{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://creative.lbl.gov/wp-content/uploads/sites/3/2020/07/6_BL_Horiz_Rev_rgb.png\"\n",
    "     width=\"500px\"\n",
    "     alt=\"LBL logo\"\n",
    "     style=\"vertical-align:middle\"/>\n",
    "\n",
    "\n",
    "# Cloud microphysics training and aerosol inference with the Fiats deep learning library\n",
    "\n",
    "**Authors:** [Damian Rouson](mailto:rouson@lbl.gov), [Zhe Bai](mailto:@zhebai@lbl.gov), [Dan Bonachea](mailto:dobonachea@lbl.gov), [Baboucarr Dibba](mailto:bdibba@lbl.gov), [Ethan Gutmann](mailto:gutmann@ucar.edu), [Katherine Rasmussen](mailto:krasmussen@lbl.gov), [David Torres](mailto:davytorres@lbl.gov), [Jordan Welsman](mailto:welsman@lbl.gov), [Yunhao Zhang](mailto:yunhao2783@gmail.com)\n",
    "\n",
    "**Keywords:** deep learning, Fortran, cloud microphysics, aerosols, surrogate model, neural network\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "This notebook presents two atmospheric sciences demonstration applications in the [Fiats](https://go.lbl.gov/fiats) deep learning software repository. The first, `train-cloud-microphysics`, trains a neural-network cloud microphysics surrogate model that has been integrated into the [Berkeley Lab fork](https://go.lbl.gov/icar) of the Intermediate Complexity Atmospheric Research (ICAR) model. The second, `infer-aerosol`, performs parallel inference with an aerosol dynamics surrogate pretrained in PyTorch using data from the Energy Exascale Earth System Model ([E<sup>3</sup>SM](https://e3sm.org)). This notebook presents the program statements involved in using Fiats for aerosol inference and microphysics training. In order to also give the interested reader direct experience with using Fiats for these purposes, the notebook details how to run two simpler example programs that serve as representative proxies for the demonstration applications.  Both proxies are also example programs in the Fiats repository. The microphysics training proxy is a self-contained example requiring no input files.  The aerosol inference proxy uses a pretrained aerosol model stored in the Fiats JavaScript Object Notation (JSON) file format and hyperlinked into this notebook for downloading, importing, and using to perform batch inference calculations with Fiats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Introduction\n",
    "### Background\n",
    "Fortran programs serve an important role in earth systems modeling from weather and climate prediction {cite}`skamarock2008description,mozdzynski2015partitioned` to wildland fire modeling {cite}`vanella2021multi` and terrestrial ecosystem simulation {cite}`shi2024functionally`.   The cost of performing ensembles of runs of such complex, multiphysics applications at scale inspires investigations into replacing model components with neural-network surrogates in such earth systems as groundwater {cite}`asher2015review` and oceans {cite}`partee2022using`.  For the surrogates to be useful, they must provide comparable or better accuracy and execution speed as the physics-based components that the surrogates replace.  The prospects for achieving faster execution depend on several properties of the surrogates, including their size, complexity, architecture, and implementation details.  Implementing the surrogates in the language of the supported application reduces interface complexity and increases runtime efficiency by reducing the need for wrapper procedures and data structure transformations.\n",
    "Inspired partly by these concerns, native Fortran software packages are emerging to support the training and deployment of neural-network surrogate models.  Two examples are neural-fortran {cite}`curcic2019parallel` and ATHENA {cite}`taylor2024athena`.  This jupyter notebook presents two demonstration applications in the repository of third example: the Fiats deep learning library {cite}`rouson2025automatically`.  One application trains a cloud microphysics surrogate for the Intermediate Complexity Atmospheric Research (ICAR) model {cite}`gutmann2016intermediate`.  The other application uses a pretrained surrogate for aerosols in the Energy Exascale Earth Systems Model (E<sup>3</sup>SM) {cite}`golaz2019doe`.\n",
    "\n",
    "Fiats, an acronym that expands to \"Functional inference and training for surrogates\" or \"Fortran inference and training for science,\" targets high-performance computing applications (HPC) in Fortran 2023. Fiats provides novel support for functional programming styles by providing inference and training procedures that have the `pure` attribute.  The Fortran standard requires that procedures invoked inside Fortran's `do concurrent` loop-parallel construct must be `pure`.  At least four compilers are currently capable of automatically parallelizing `do concurrent` on central processing units (CPUs) or graphics processing units (GPUs): the Intel Fortran Compiler `ifx`, LLVM `flang`, the NVIDIA HPC Fortran compiler `nvfortran`, and the HPE Cray Compiling Environment Fortran compiler `crayftn`. Providing `pure` inference and training procedures thus supports parallel programming across compilers and hardware. \n",
    "When compiled with `flang`, for example, Fiats offers automatic parallelization of batch inference calculations with strong scaling trends comparable to those achievable with OpenMP directives {cite}`rouson2025automatically`.  These results were obtained by building LLVM `flang` from source using the `paw-atm24-fiats` git tag on the [Berkeley Lab LLVM fork](https://github.com/BerkeleyLab/flang-testing-project).  The relevant features were merged into the [llvm-project](https://github.com/llvm/llvm-project) main branch in April 2025 and will be part of the LLVM 21 release.   Work is also under way to support `ifx`.\n",
    "\n",
    "Fiats provides a derived type that encapsulates neural-network parameters and provides generic bindings `infer` and `train` for invoking inference functions and training subroutines, respectively, of various precisions.  A novel feature of the Fiats design is that all procedures involved in inference and training have the `non_overridable` attribute, which eliminates the need for dynamic dispatch at call sites. In addition to simplifying the structure of the resulting executable program and potentially improving performance, we expect this feature to enable the automatic offload of inference and training to GPUs.\n",
    "\n",
    "What most distinguishes Fiats from similar projects is the Fiats project's dual purposes as a platform for deep learning research and for exploring how novel programming patterns in Fortran an 2023 can benefit deep learning.   Our close collaboration with compiler developers leads to innovations such as the aforementioned use of the `non_overridable` attribute, which we did not find anywhere in the other aforementioned Fortran deep learning libraries. `Non_overridable` is the linchpin in efficiently combining functional, object-oriented, and parallel programming, i.e., combining `pure`, `class`, and `do concurrent`, respectively.  To see the connection, consider that Fortran requires the passed-object dummy argument (the object on which a type-bound procedure is invoked) to be declared with the keyword `class`, which nominally connotes polymorphism.  Using `class` allows the dynamic type of the object to vary from one invocation to the next.  Polymorphism, one of the central pillars of object-oriented programming, drives the need for dynamic dispatch: compilers must create executable programs that can make a runtime decision about whether to invoke the version of a procedure defined by the type named in the declaration or to invoke a version of the procedure provided by a child type that overrides the procedure.  Eliminating the need for this decision avoids unnecessary overhead on CPUs and is essential on GPUs.  Neither LLVM `flang` nor its C++ counterpart, `clang++`, supports dynamic dispatch on GPUs.\n",
    "\n",
    "### Objectives\n",
    "The primary objectives of this notebook are to describe the use of Fiats in the `infer-aerosol` and `train-cloud-microphysics` demonstration applications and to provide the user experience with Fiats by detailing how to run simpler example programs that use Fiats in similar ways to the demonstration applications and thus serve as reasonable proxies for the demonstration applications.  The aerosol inference proxy performs batch inference calculations using a pretrained aerosol model {cite}`bai2024deep` provided via hyperlink in this notebook. The cloud microphysics proxy trains a deep neural network to model an ICAR SB04 cloud microphysics model function that predicts the saturated mixing ratio from a given pressure and temperature.  The [Methodology](#methodology) section of this notebook describes the use of Fiats in the two demonstration applications.  The [Discussion](#discussion) section explains how to run the two proxies.   We expect that the reader familiar with recent Fortran standards will learn the program statements required to use Fiats.  We further expect that an interested reader who installs the prerequisite build system and compiler will learn to run programs locally using Fiats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "### Getting started\n",
    "With the `tree` utility installed, the following `bash` shell commands will download the Fiats repository, checkout the `git` commit used in writing this notebook, and show the Fiats directory tree:\n",
    "```bash\n",
    "git clone --branch sea-iss-2025 git@github.com:berkeleylab/fiats \n",
    "cd fiats\n",
    "tree -d\n",
    ".\n",
    "├── demo\n",
    "│   ├── app\n",
    "│   ├── include -> ../include\n",
    "│   ├── src\n",
    "│   └── test\n",
    "├── doc\n",
    "│   └── uml\n",
    "├── example\n",
    "│   └── supporting-modules\n",
    "├── include\n",
    "├── scripts\n",
    "├── src\n",
    "│   └── fiats\n",
    "└── test\n",
    "```\n",
    "where the `src` directory contains the source comprising the Fiats library that programs link against to access the Fiats library's data entities and  procedures.  As such, `src` contains the only files that a Fiats user needs and it contains no main programs.  The `fiats_m` `module`in `src/fiats_m.f90` exposes all user-facing Fiats functions, subroutines, derived types, and constants.  The `src/fiats` subdirectory contains the definitions of everything in `fiats_m` as well as private internal implementation details.  For a program to access Fiats entities, it would suffice for a `use fiats_m` statement to appear in any program unit or subprogram that requires Fiats.\n",
    "\n",
    "Apart from the library, the Fiats `git` repository contains main programs that demonstrate how to use Fiats.  The main programs are in two subdirectories: \n",
    "1. `example/` contains relatively short and mostly self-contained programs and\n",
    "2. `demo/app/` contains demonstration applications developed with collaborators for production use.\n",
    "The next two subsections describe the demonstration applications.  Using the demonstration applications requires several problem-specific files and prerequisite software packages.  Using `train-cloud-microphysics` also requires considerable resources in the form of thousands of core-hours of ICAR runs to produce hundreds of gigabytes of data.  To give the reader a gentler introduction to using Fiats, the [Discussion](#discussion) section describes how to use the example programs only.\n",
    "\n",
    "### Demonstration application: aerosol inference\n",
    "The `demo/app/infer-aerosol.f90` program demonstrates the use of Fiats to predict aerosol variables for E<sup>3</sup>SM.   The following statement provides access to all Fiats entities employed by the program:\n",
    "```fortran\n",
    "   use fiats_m, only : unmapped_network_t, tensor_t, double_precision, double_precision_file_t\n",
    "```\n",
    "where the `unmapped_network_t` derived type encapsulates a neural network that performs no mappings on input and output tensors, `tensor_t` encapsulates network input and output tensors, `double_precision` is a kind type parameter that specifies the desired precision, and the `double_precision_file_t`  derived type provides a file abstraction that determines how numerical values in model files will be interpreted.  Because Fiats focuses on surrogate models that must be compact in order to be competitive with the physics-based models they replace, Fiats uses a JSON file format for its human readability because we have found the ability to inspect network parameters visually helpful in the early stages of experimenting with new algorithms.  Users with models trained in PyTorch can use the Fiats companion network export software [Nexport](https://github.com/berkeleylab/nexport) to export models to the Fiats JSON format.\n",
    "\n",
    "After chores such as printing usage information if a user omits a required command-line argument, the following object declaration demonstrates the first direct use of Fiats in the program: \n",
    "```fortran\n",
    "   type(unmapped_network_t(double_precision)) neural_network\n",
    "```\n",
    "Fiats uses derived type parameters -- specifically kind type parameters -- so that one neural-network type can be used to declare objects with any supported `kind` parameter.  Currently, the supported `kind` parameters are `default_real` and `double_precision`, corresponding to the chosen compiler's `real` (with no specified `kind` parameter) and `double precision` types.  Fiats types with a kind type parameter provide a default initialization of the parameter to `default_real`.  \n",
    "\n",
    "A later line defines the object:\n",
    "```fortran\n",
    "   neural_network = unmapped_network_t(double_precision_file_t(path // network_file_name))\n",
    "```\n",
    "where `unmapped_network_t` appears in this context as a generic interface patterned after Fortran's structure constructors that define new objects.  Because the JSON specification does not differentiate types of numbers (e.g., JSON does not distinguish integers from real numbers), using the Fiats `double_precision_file_t` type specifies how to interpret values read from the model file.\n",
    "\n",
    "Similarly, the later line:\n",
    "```fortran\n",
    "   type(tensor_t(double_precision)), allocatable :: inputs(:), outputs(:)\n",
    "```\n",
    "specifies the precision used for tensor objects, and the `tensor_t` generic interface in the following statement:\n",
    "```fortran\n",
    "   inputs(i) = tensor_t(input_components(i,:))\n",
    "```\n",
    "resolves to an invocation of a function that produces a double-precision object because of a declaration earlier in the code (not shown here) that declares `input_components` to be of type `double precision`.  Ultimately, inference happens by invoking a type-bound `infer` procedure on the `neural_network` object and providing `tensor_t` input objects to produce the corresponding `tensor_t` output objects:\n",
    "```fortran\n",
    "   !$omp parallel do shared(inputs,outputs,icc)\n",
    "   do i = 1,icc\n",
    "     outputs(i) = neural_network%infer(inputs(i))\n",
    "   end do\n",
    "   !$omp end parallel do\n",
    "```\n",
    "where we parallelize the loop using OpenMP directives.  Alternatively, the Fiats `example/concurrent-inferences.F90` program invokes `infer` inside a `do concurrent` construct, taking advantage of `infer` being `pure`.  This approach has the advantage that compilers can automatic parallelize the iterations without OpenMP directives.  Besides simplifying the code, switching to `do concurrent` means the exact same source code can run in parallel on a CPU or a GPU without change.  With most compilers, switching from running on one device to another requires simply recompiling with different flags.  See {cite:t}`rouson2025automatically`  for more details on automatically parallelizing inference, including strong scaling results on one node of the Perlmutter supercomputer at the National Energy Research Scientific Computing ([NERSC](https://www.nersc.gov)) Center.\n",
    "\n",
    "The remainder of `infer-aerosol` contains problem-specific statements not directly related to the use of Fiats and is therefore beyond the scope of this notebook.\n",
    "\n",
    "### Demonstration application: microphysics training\n",
    "Training a neural network is an inherently more involved process than using a neural network for inference.  As such, `train-cloud-microphysics` uses a larger number of Fiats entities:\n",
    "```fortran\n",
    "use fiats_m, only : tensor_t, trainable_network_t, input_output_pair_t, mini_batch_t, &\n",
    "    tensor_map_t, training_configuration_t, training_data_files_t, shuffle\n",
    "```\n",
    "where only the `tensor_t` type intersects with the set of entities that `infer-aerosols` uses.  The remaining entities in the above `use` statement all relate to training neural networks.\n",
    "\n",
    "The `trainable_network_t` type extends the `neural_network_t` type and thus offers the same type-bound procedures by inheritance. \n",
    "Outwardly, `trainable_network_t` differs from `neural_network_t` only in that `trainable_network_t` provides public `train` and `map_to_training_ranges` generic bindings that `neural_network_t` lacks.  Calling `train` performs a forward pass followed by a back-propagation pass that adjusts the neural-network weights and biases.  If the network input and output ranges for training differ from the corresponding tensor values for the application (e.g., we often find it useful to map input tensor values to the unit interval [0,1] for training), then calling `map_to_training_ranges` performs the desired transformation and the resulting `tensor_map_t` type encapsulates the forward and inverse mappings.  Privately, the `trainable_network_t` type stores a `workspace_t` object containing a training scratchpad that gets dynamically sized in a way that is invisible to Fiats users.  Hiding this implementation detail without forcing the `neural_network_t` type to have components needed only for training is the primary reason that `trainable_network_t` exists.\n",
    "\n",
    "The `input_output_pair_t` derived type encapsulates training-data pairings ensuring a one-to-one connection between `tensor_t` inputs and outputs as required for supervised learning {cite}`goodfellow2016deep`.  The `mini_batch_t` type supports the formation of `input_output_pair_t` subgroups. The ability to form mini-batches and to randomly shuffle the composition of mini-batches via the listed `shuffle` subroutine combine to facilitate the implementation of the foundational stochastic gradient descent optimization algorithm for training.\n",
    "\n",
    "Finally, the `training_configuration_t` and `training_data_files_t` types encapsulate file formats that Fiats users employ to define training hyperparameters (e.g., learning rate) and to specify the collection of files that contain training data.  With all of the aforementioned derived types in place, `train-cloud-microphysics` uses a capability of the external Julienne framework {cite}`julienne` to group the training data into bins:\n",
    "```fortran\n",
    "   bins = [(bin_t(num_items=num_pairs, num_bins=n_bins, bin_number=b), b = 1, n_bins)]\n",
    "```\n",
    "and then these bins are shuffled into new mini-batch subsets at the beginning of each epoch:\n",
    "\n",
    "```fortran\n",
    "do epoch = first_epoch, last_epoch\n",
    "\n",
    "  if (size(bins)>1) call shuffle(input_output_pairs) ! set up for stochastic gradient descent\n",
    "    mini_batches = [(mini_batch_t(input_output_pairs(bins(b)%first():bins(b)%last())), b = 1, size(bins))]\n",
    "\n",
    "    call trainable_network%train(mini_batches, cost, adam, learning_rate)\n",
    "```\n",
    "where `cost` is an `intent(out)` variable containing the cost function calculated at the end of an epoch; `adam`\n",
    "is a `logical` `intent(in)` variable that switches on the Adam optimizer {cite}`kingma2017adam`; and `learning_rate` is an `intent(in)` variable that scales the adjustments to the model weights and biases. This completes the presentation of essential Fiats capabilities employed by `train-cloud-microphysics`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "This section aims to provide the interested reader with experience in running programs that use Fiats for predicting atmospheric aerosol dynamics and for training a cloud microphysics model. \n",
    "\n",
    "### Build system and compiler installation\n",
    "Building Fiats requires the Fortran Package Manager ([`fpm`](https://github.com/fortran-lang/fpm)) and the LLVM [`flang`](https://github.com/llvm/llvm-project) Fortran compiler.  The \"package manager\" part of `fpm`'s name refers to `fpm`'s ability to download, if necessary, and build a project's dependencies if those dependencies are also `fpm` projects.  Because LLVM is not an `fpm` project, `fpm` cannot build `flang`.\n",
    "\n",
    "\n",
    "The commands in this notebook were tested with `fpm` 0.11.0 and `flang` 20.1.2.  It might be easiest to install `fpm` and `flang` using package managers -- for example, using the [Homebrew](https://brew.sh) command `brew install fpm flang` on macOS or the command `sudo apt install fortran-fpm flang-20` on Ubuntu Linux.  Alternatively, if you have the GNU Compiler Collection ([GCC](https://gcc.gnu.org)) `gfortran` compiler, an especially easy way to install `fpm` is to compile the single file that each `fpm` release offers with all of `fpm`'s source code (which is written in Fortran) concatenated into one file.  On the `fpm` 0.11.0 release page, for example, that file is [fpm-0.11.0.F90](https://github.com/fortran-lang/fpm/releases/download/v0.11.0/fpm-0.11.0.F90).  Downloading that file to an otherwise empty directory (because compiling it produces many files, all but one of which can be deleted after compiling), compiling with a command such as `gfortran -o fpm -fopenmp fpm-0.11.0.F90`, and moving the resulting `fpm` executable file to a directory in your `PATH` gives you a working installation of `fpm`.  In the provided single-file compile command, `-fopenmp` enables `fpm` to shorten compile times via parallel builds.\n",
    "\n",
    "With `fpm` and `flang` installed and the repository cloned as explained in the [Getting started](#getting-started) section of this notebook, and with your present working directory set to anywhere in the clone, you can build and test Fiats with the command:\n",
    "```bash\n",
    "fpm test --compiler flang-new --flag -O3\n",
    "```\n",
    "where we invoke `flang` via the alias `flang-new` for historical reasons.  We expect that an upcoming version of `fpm` will recognize `flang` as LLVM `flang`.  If everything succeeds, the trailing output from the latter command should be:\n",
    "```\n",
    "_________ In total, 37 of 37 tests pass. _________\n",
    "```\n",
    "at which point it will be possible to run any of the programs in the Fiats `example/` subdirectory.\n",
    "\n",
    "### Running microphysics training\n",
    "As explained in the [Objectives](#objectives) section of this notebook, this section uses a proxy for the much more involved `train-cloud-microphysics` demonstration application.  The proxy is the `example/learn-saturated-mixing-ratio` program.  The proxy trains a neural-network surrogate to represent the function defined in `example/supporting-modules/saturated-mixing-ratio.f90`.  The function result is the saturated mixing ratio, a thermodynamic variable corresponding to the maximum amount of water vapor that the air at a given location can hold in the gaseous state without condensing to liquid.   The function has two arguments: temperature and pressure. The function was extracted from ICAR's SB04 simple microphysics model, refactored to accept arguments mapped to the unit interval [0,1], and then wrapped by a function that accepts and returns `tensor_t` objects from which the Fiats `infer` function extracts model inputs and the corresponding outputs.  The resulting trained neural network stores the physics-based (SB04) model input and output extrema for purposes of capturing the mapping function that transforms data between the application data range and the model data range ([0,1]). \n",
    "\n",
    "The `learn-saturated-mixing-ratio` example program uses Fiats entities that form a subset of those used by `train-cloud-microphysics`:\n",
    "```fortran\n",
    "   use fiats_m, only : trainable_network_t, mini_batch_t, tensor_t, input_output_pair_t, shuffle\n",
    "```\n",
    "which demonstrates why the example is a simpler proxy for the demonstration application. Because the [Methods](#methods) section of this notebook details the demonstration application's use of Fiats, the current section skips over example program details and instead focuses on running the example.\n",
    "\n",
    "When compiled and run without command-line arguments, each program in the Fiats `example/` and `demo/app/` subdirectories prints helpful usage information.  Hence, running the provided programs without arguments is one way to find out the required arguments: \n",
    "```bash\n",
    "   fpm run \\\n",
    "     --example learn-saturated-mixing-ratio \\\n",
    "     --compiler flang-new \\\n",
    "     --flag -O3\n",
    "```\n",
    "which generates the trailing output:\n",
    "```bash\n",
    "Fortran ERROR STOP: \n",
    "\n",
    "Usage:\n",
    "  fpm run \\\n",
    "  --example learn-saturated-mixing-ratio \\\n",
    "  --compiler flang-new \\\n",
    "  --flag \"-O3\" \\\n",
    "  -- --output-file \"<file-name>\"\n",
    "\n",
    "<ERROR> Execution for object \" learn-saturated-mixing-ratio \" returned exit code  1\n",
    "<ERROR> *cmd_run*:stopping due to failed executions\n",
    "STOP 1\n",
    "```\n",
    "where `fpm` commands place command-line arguments for the running program after double-dashes (`--`), so the above message indicates that the program requires command-line arguments of the form `--output-file \"<file-name>\"`.\n",
    "\n",
    "The following command runs the program again with the required argument:\n",
    "```bash\n",
    "   fpm run \\\n",
    "     --compiler flang-new \\\n",
    "     --flag -O3 \\\n",
    "     --example learn-saturated-mixing-ratio \\\n",
    "     -- --output-file saturated-mixing-ratio.json \n",
    "```\n",
    "which should produce leading output like the following:\n",
    "```\n",
    " Initializing a new network\n",
    "         Epoch | Cost Function| System_Clock | Nodes per Layer\n",
    "         1000    0.77687E-04     6.0089      2,4,72,2,1\n",
    "         2000    0.60092E-04     12.062      2,4,72,2,1\n",
    "         3000    0.45148E-04     18.110      2,4,72,2,1\n",
    "         4000    0.33944E-04     24.253      2,4,72,2,1\n",
    "```\n",
    "showing the cost function decreasing with increasing numbers of epochs for a neural network that accepts `2` inputs (normalized pressure and temperature) and uses three hidden layers of width `4`, `72`, and `2` to produce `1` output (saturated mixing ratio).\n",
    "\n",
    "The program uses a cost-function tolerance of `1.0E-08`, which takes a very long time to attain when compiled with LLVM `flang` 20.  (An earlier version of Fiats, 0.14.0, supports compiling with `gfortran`, which produces executable programs that run approximately 20x faster.  We anticipate supporting `gfortran` again in a future release after bugs in `gfortran`'s support for kind type parameters have been fixed. We also anticipate that future releases of LLVM `flang`, one of the newest Fortran compilers, will improve in the ability to optimize code and generate faster programs.)\n",
    "\n",
    "To gracefully shut down the example program, issue the command `touch stop`, which creates an empty file named `stop`.  The program periodically checks for this file, halts execution if the file exists, and prints a table of model inputs along with the actual and desired model outputs.  [Fig. 1](sat-mix-rat) compares the surrogate and physics-based model output surfaces over the unit square domain $[0,1]^2$. It demonstrates that the two surfaces are visually indistinguishable, except that whichever of the two surface colors shows at a given point indicates which surface would be visible from the given viewing angle.  Viewed from above, for example, the color corresponding to whichever surface is slightly higher than the other shows.  From below, whichever is slightly lower shows.\n",
    "\n",
    "```{figure} ../assets/saturated-mixing-ratio-surface-plot2.png\n",
    ":name: sat-mix-rat\n",
    ":align: center\n",
    "\n",
    "Surface plots of the actual (green) and desired (blue) saturated mixing ratio model outputs.\n",
    "```\n",
    "\n",
    "### Running aerosol inference\n",
    "This section of the notebook uses the `concurrent-inferences` program in the `example/` subdirectory as a proxy for the more involved `infer-aerosol` demonstration application in the `demo/app` subdirectory.  This example program performs batches of inferences taking a three-dimensional (3D) array of `tensor_t` input objects and producing a 3D array of `tensor_t` output objects.  The sizes of the 3D arrays is representative of the grids used in an ICAR production run. \n",
    "\n",
    "Run the proxy with no command-line arguments for the program itself:\n",
    "```bash\n",
    "   fpm run --example concurrent-inferences --compiler flang-new --flag -O3\n",
    "```\n",
    "which should yield the usage information:\n",
    "```bash\n",
    "Usage:\n",
    "  fpm run \\\n",
    "    --example concurrent-inferences \\\n",
    "    --compiler flang-new --flag -O3 \\\n",
    "    -- --network \"<file-name>\" \\\n",
    "    [--do-concurrent] [--openmp] [--elemental] [--double-precision] [--trials <integer>]\n",
    "where <> indicates user input and [] indicates an optional argument.\n",
    "```\n",
    "where the first three optional arguments specify a strategy for iterating across the batch: Fortran's loop-parallel `do concurrent` construct, OpenMP multithreading, or an `elemental` procedure that operates on whole `tensor_t` arrays or array slices in one `infer` invocation.  If none of the first three optional arguments exists on the command line, then all three execute.  The fourth optional argument decides whether to additionally perform inference using double precision. The final optional argument determines the number of times each strategy will execute.  See {cite}`rouson2025automatically` for a discussion of the performance of each of these approaches.\n",
    "\n",
    "Before running `concurrent-inferences`, download the pretrained aerosol model file [model.json](./assets/model.json) and save it to the root directory of your Fiats clone. \n",
    "To import the pretrained model and run the program, enter the following command:\n",
    "```bash\n",
    "  fpm run \\\n",
    "    --example concurrent-inferences \\\n",
    "    --compiler flang-new --flag -O3 \\\n",
    "    -- --network model.json\n",
    "```\n",
    "which should yield trailing output of the form:\n",
    "```\n",
    " Constructing a new neural_network_t object from the file model.json\n",
    " Defining an array of tensor_t input objects with random normalized components\n",
    " Performing 1250565  inferences inside `do concurrent`.\n",
    " Elapsed system clock during `do concurrent` inference:  42.464405\n",
    " Performing 1250565  inferences inside `omp parallel do`.\n",
    " Elapsed system clock during `OpenMP` inference:  43.313417\n",
    " Performing elemental inferences inside `omp workshare`\n",
    " Elapsed system clock during `elemental` inference:  41.631988\n",
    " Constructing a new neural_network_t object from the file model.json\n",
    " Defining an array of tensor_t input objects with random normalized components\n",
    " Performing double-precision inference inside `do concurrent`\n",
    " Elapsed system clock during double precision concurrent inference:  66.173744\n",
    " variable          mean           stdev\n",
    " t_dc     42.464405 0.\n",
    " t_omp    43.313417 0.\n",
    " t_elem   41.631988 0.\n",
    " t_dp_dc  66.173744 0.\n",
    "```\n",
    "In interpreting these timings, note that Homebrew installs LLVM `flang` without OpenMP support and `flang`'s capability for automatically parallelizing of `do concurrent` did not make it into `flang` version 20, but should appear in `flang` version 21.  Lastly, compilers do not yet parallelize array statements inside OpenMP's `!$omp workshare` blocks.  {cite}`rouson2025automatically` present results with OpenMP support and with automatic parallelization of `do concurrent` enabled.\n",
    "\n",
    "[Fig. 2](aerosol-viz) visualizes predictions of the accumulation-mode aerosol concentration made by the aerosol model that we used with the `infer-aerosol` demonstration application.  The visualization is produced by software unrelated to Fiats and is provided for purposes of understanding the model data the model produces.\n",
    "```{figure} ../assets/ncvis_output_0104_a1.png\n",
    ":name: aerosol-viz\n",
    ":align: center\n",
    "\n",
    "A global view of the concentration of accumulation-mode aerosol particles as predicted by the model used in this section of the notebook.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "This notebook introduces the reader to program statements that demonstration applications can use to access Fiats functionality for training a cloud microphysics surrogate model, or to perform batch inference using an aerosol surrogate.  After providing information on installing the requisite compiler (LLVM `flang`) and build system (`fpm`), the notebook  walks the reader through the process of running simplified example proxy programs modeled after the demonstration applications. Related visualizations demonstrate  the accuracy of the microphysics model trained by Fiats and the richness of the structures representable with the pretrained aerosol model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "```{bibliography}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
