# Introduction
## Background
Fortran programs serve an important role in earth systems modeling from weather and climate prediction {cite}`skamarock2008description,mozdzynski2015partitioned` to wildland fire modeling {cite}`vanella2021multi` and terrestrial ecosystem simulation {cite}`shi2024functionally`.  The cost of performing ensembles of runs of such complex, multiphysics applications at scale inspires investigations into replacing model components with neural-network surrogates in such earth systems as groundwater {cite}`asher2015review` and oceans {cite}`partee2022using`.  For the surrogates to be useful, they must provide comparable or better accuracy and execution speed as the physics-based components that the surrogates replace.  The prospects for achieving faster execution depend on several properties of the surrogates, including their size, complexity, architecture, and implementation details.  Implementing the surrogates in the language of the supported application reduces interface complexity and increases runtime efficiency by reducing the need for wrapper procedures and data structure transformations.
Inspired partly by these concerns, native Fortran software packages are emerging to support the training and deployment of neural-network surrogate models.  Two examples are neural-fortran {cite}`curcic2019parallel` and ATHENA {cite}`taylor2024athena`.  This Jupyter notebook presents two demonstration applications in the repository of third example: the Fiats deep learning library {cite}`rouson2025automatically`.  One application trains a cloud microphysics surrogate for the Intermediate Complexity Atmospheric Research (ICAR) model {cite}`gutmann2016intermediate`.  The other application uses a pretrained surrogate for aerosols in the Energy Exascale Earth Systems Model (E<sup>3</sup>SM) {cite}`golaz2019doe`.

Fiats, an acronym that expands to "Functional inference and training for surrogates" or "Fortran inference and training for science," targets high-performance computing applications (HPC) in Fortran 2023. Fiats provides novel support for functional programming styles by providing inference and training procedures that have the `pure` attribute.  The Fortran standard requires that procedures invoked inside Fortran's `do concurrent` loop-parallel construct must be `pure`.  At least four compilers are currently capable of automatically parallelizing `do concurrent` on central processing units (CPUs) or graphics processing units (GPUs): the Intel Fortran Compiler `ifx`, LLVM `flang`, the NVIDIA HPC Fortran compiler `nvfortran`, and the HPE Cray Compiling Environment Fortran compiler `crayftn`. Providing `pure` inference and training procedures thus supports parallel programming across compilers and hardware. 
When compiled with `flang`, for example, Fiats offers automatic parallelization of batch inference calculations with strong scaling trends comparable to those achievable with OpenMP directives {cite}`rouson2025automatically`.  These results were obtained by building LLVM `flang` from source using the `paw-atm24-fiats` git tag on the [Berkeley Lab LLVM fork](https://github.com/BerkeleyLab/flang-testing-project).  The relevant features were merged into the [llvm-project](https://github.com/llvm/llvm-project) main branch in April 2025 and will be part of the LLVM 21 release.  Work is also under way to support `ifx`.

Fiats provides a derived type that encapsulates neural-network parameters and provides generic bindings `infer` and `train` for invoking inference functions and training subroutines, respectively, of various precisions.  A novel feature of the Fiats design is that all procedures involved in inference and training have the `non_overridable` attribute, which eliminates the need for dynamic dispatch at call sites. In addition to simplifying the structure of the resulting executable program and potentially improving performance, we expect this feature to enable the automatic offload of inference and training to GPUs.

What most distinguishes Fiats from similar projects is the Fiats project's dual purposes as a platform for deep learning research and for exploring how novel programming patterns in Fortran an 2023 can benefit deep learning.  Our close collaboration with compiler developers leads to innovations such as the aforementioned use of the `non_overridable` attribute, which we did not find anywhere in the other aforementioned Fortran deep learning libraries. `Non_overridable` is the linchpin in efficiently combining functional, object-oriented, and parallel programming, i.e., combining `pure`, `class`, and `do concurrent`, respectively.  To see the connection, consider that Fortran requires the passed-object dummy argument (the object on which a type-bound procedure is invoked) to be declared with the keyword `class`, which nominally connotes polymorphism.  Using `class` allows the dynamic type of the object to vary from one invocation to the next.  Polymorphism, one of the central pillars of object-oriented programming, drives the need for dynamic dispatch: compilers must create executable programs that can make a runtime decision about whether to invoke the version of a procedure defined by the type named in the declaration or to invoke a version of the procedure provided by a child type that overrides the procedure.  Eliminating the need for this decision avoids unnecessary overhead on CPUs and is essential on GPUs.  Neither LLVM `flang` nor its C++ counterpart, `clang++`, supports dynamic dispatch on GPUs.

## Objectives
The primary objectives of this notebook are to describe the use of Fiats in the `infer-aerosol` and `train-cloud-microphysics` demonstration applications and to provide the user experience with Fiats by detailing how to run simpler example programs that use Fiats in similar ways to the demonstration applications and thus serve as reasonable proxies for the demonstration applications.  The aerosol inference proxy performs batch inference calculations using a pretrained aerosol model {cite}`bai2024deep` provided via hyperlink in this notebook. The cloud microphysics proxy trains a deep neural network to model an ICAR SB04 cloud microphysics model function that predicts the saturated mixing ratio from a given pressure and temperature.  The [Methodology](#methodology) section of this notebook describes the use of Fiats in the two demonstration applications.  The [Discussion](#discussion) section explains how to run the two proxies.  We expect that the reader familiar with recent Fortran standards will learn the program statements required to use Fiats.  We further expect that an interested reader who installs the prerequisite build system and compiler will learn to run programs locally using Fiats.


